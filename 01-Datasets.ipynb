{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:24:39.149871Z",
     "start_time": "2021-06-24T19:24:39.128899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\user\\\\Research\\\\acora-pure',\n",
       " 'E:\\\\GoogleDrive\\\\acora-data',\n",
       " 'E:\\\\Research\\\\Datasets\\\\BERT')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get paths set either from environment variables or if not set use some default values\n",
    "import os\n",
    "\n",
    "if 'ACORA_HOME_PATH' in os.environ:\n",
    "    acora_home_path = os.environ['ACORA_HOME_PATH']\n",
    "else:\n",
    "    acora_home_path = \"../../acora\"\n",
    "\n",
    "if 'ACORA_DATA_PATH' in os.environ:\n",
    "    data_path = os.environ['ACORA_DATA_PATH']\n",
    "else:\n",
    "    data_path = \"./data\"\n",
    "    \n",
    "if 'BERT_PRETRAIN_MODELS_PATH' in os.environ:\n",
    "    berts_pretrain_path = os.environ['BERT_PRETRAIN_MODELS_PATH']\n",
    "else:\n",
    "    berts_pretrain_path = \"../bert\"\n",
    "\n",
    "acora_home_path, data_path, berts_pretrain_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:27:08.466232Z",
     "start_time": "2021-06-24T19:26:59.472681Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "\n",
    "import warnings  \n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    if tf.__version__.startswith(\"1.\"):\n",
    "        os.environ['TF_KERAS'] = '0'\n",
    "        import keras\n",
    "    else:\n",
    "        os.environ['TF_KERAS'] = '1'\n",
    "        import tensorflow.compat.v1.keras as keras\n",
    "        tf.get_logger().setLevel('INFO')\n",
    "         \n",
    "    from keras_bert import Tokenizer, load_trained_model_from_checkpoint\n",
    "\n",
    "\n",
    "from acora.vocab import BERTVocab\n",
    "from acora.comments import default_subject_columns, \\\n",
    "    load_comments_files, CommentPurposeTransformer, CommentSubjectTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:27:37.649237Z",
     "start_time": "2021-06-24T19:27:37.643236Z"
    }
   },
   "outputs": [],
   "source": [
    "#bert_name = 'uncased_L-8_H-512_A-8'\n",
    "bert_name = 'multi_cased_L-12_H-768_A-12'\n",
    "\n",
    "config_path = os.path.join(berts_pretrain_path, bert_name, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(berts_pretrain_path, bert_name, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(berts_pretrain_path, bert_name, 'vocab.txt')\n",
    "with open(config_path, \"r\", encoding='utf', errors='ignore') as json_file:\n",
    "    bert_config = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:27:39.963228Z",
     "start_time": "2021-06-24T19:27:39.768230Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = BERTVocab.load_from_file(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:27:40.873262Z",
     "start_time": "2021-06-24T19:27:40.855235Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab.token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:29:07.816605Z",
     "start_time": "2021-06-24T19:29:07.811605Z"
    }
   },
   "outputs": [],
   "source": [
    "sep = \"$\"\n",
    "\n",
    "line_column = \"line_contents\"\n",
    "message_column = \"message\"\n",
    "purpose_column = \"purpose\"\n",
    "subject_columns = default_subject_columns\n",
    "\n",
    "cols = [line_column, message_column, purpose_column] + subject_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wireshark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:29:37.069605Z",
     "start_time": "2021-06-24T19:29:36.314606Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from E:\\GoogleDrive\\acora-data\\wireshark\\wireshark_comments_all.xlsx\n",
      "Loaded 1,248 rows and 15 cols...\n"
     ]
    }
   ],
   "source": [
    "training_data_paths = [\n",
    "    os.path.join(data_path, \"wireshark\", \"wireshark_comments_all.xlsx\")\n",
    "]\n",
    "wireshark_df = load_comments_files(training_data_paths, cols, sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are any duplicated comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:29:53.890604Z",
     "start_time": "2021-06-24T19:29:53.497606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments = 1248\n",
      "Number of unique comments = 946\n",
      "Number of duplicated lines and comments = 73\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of comments = {wireshark_df.shape[0]}\")\n",
    "print(f\"Number of unique comments = {wireshark_df[message_column].unique().shape[0]}\")\n",
    "print(f\"Number of duplicated lines and comments = {pd.concat(g for _, g in wireshark_df.groupby([line_column, message_column]) if len(g) > 1).shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:33:54.341345Z",
     "start_time": "2021-06-24T19:33:53.879346Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\t\t\tDISSECTOR_ASSERT(pdata != NULL)_' ---> 'Done'\n",
      "'\t\t\tDISSECTOR_ASSERT(pdata != NULL)_' ---> 'Done'\n",
      "'    ' ---> 'Done'\n",
      "'    ' ---> 'Done'\n",
      "'                           pipename, lastError, win32strerror(lastError))_' ---> 'Done'\n",
      "'                           pipename, lastError, win32strerror(lastError))_' ---> 'Done'\n",
      "'                   lastError, win32strerror(lastError))_' ---> 'And here.'\n",
      "'                   lastError, win32strerror(lastError))_' ---> 'And here.'\n",
      "'                   lastError, win32strerror(lastError))_' ---> 'Done'\n",
      "'                   lastError, win32strerror(lastError))_' ---> 'Done'\n",
      "'            enable_export = false_' ---> 'Done'\n",
      "'            enable_export = false_' ---> 'Done'\n",
      "'            enable_export = false_' ---> 'Is this code still in use? I think we disabled the entry in this cases.'\n",
      "'            enable_export = false_' ---> 'Is this code still in use? I think we disabled the entry in this cases.'\n",
      "'        {' ---> 'Done'\n",
      "'        {' ---> 'Done'\n",
      "'        {' ---> 'Is this code still in use? I think we disabled the entry in this cases.'\n",
      "'        {' ---> 'Is this code still in use? I think we disabled the entry in this cases.'\n",
      "'        }' ---> 'Done'\n",
      "'        }' ---> 'Done'\n",
      "'        }' ---> 'Is this code still in use? I think we disabled the entry in this cases.'\n",
      "'        }' ---> 'Is this code still in use? I think we disabled the entry in this cases.'\n",
      "'    # It could also be empty if the default include dir is in use.' ---> 'Ok, updated comment.'\n",
      "'    # It could also be empty if the default include dir is in use.' ---> 'Ok, updated comment.'\n",
      "'    # It could also be empty if the default include dir is in use.' ---> 'True for pkg-config, not true for pcap-config.  pcap-config is just a script generated by autoconf or CMake, and the echo commands that print the output of pcap-config --cflags and pcap-config --cflags --libs always have a -I flag, and the argument to -I is the shell variable \"includedir\", and that's always set to a pathname.'\n",
      "'    # It could also be empty if the default include dir is in use.' ---> 'True for pkg-config, not true for pcap-config.  pcap-config is just a script generated by autoconf or CMake, and the echo commands that print the output of pcap-config --cflags and pcap-config --cflags --libs always have a -I flag, and the argument to -I is the shell variable \"includedir\", and that's always set to a pathname.'\n",
      "'    PROTO_ITEM_SET_GENERATED(cmd_ref_item)_' ---> 'Done'\n",
      "'    PROTO_ITEM_SET_GENERATED(cmd_ref_item)_' ---> 'Done'\n",
      "'    dataCoding = tvb_get_guint8(tvb, offset-1)_' ---> 'the DCS should be returned by smpp_handle_dcs()'\n",
      "'    dataCoding = tvb_get_guint8(tvb, offset-1)_' ---> 'the DCS should be returned by smpp_handle_dcs()'\n",
      "'    {  14,  14, \"Ethernet Line(EPL Type 2)\" },' ---> 'Done'\n",
      "'    {  14,  14, \"Ethernet Line(EPL Type 2)\" },' ---> 'Done'\n",
      "'  if (mapping != NULL) {' ---> 'why test the pointer? It cannot be NULL'\n",
      "'  if (mapping != NULL) {' ---> 'why test the pointer? It cannot be NULL'\n",
      "'  if (mapping != NULL) {' ---> 'why test the pointer? It cannot be NULL'\n",
      "'  if (mapping != NULL) {' ---> 'why test the pointer? It cannot be NULL'\n",
      "'  if (mapping != NULL) {' ---> 'why test the pointer? It cannot be NULL'\n",
      "'  if (mapping != NULL) {' ---> 'why test the pointer? It cannot be NULL'\n",
      "'  if (mapping != NULL) {' ---> 'why test the pointer? It cannot be NULL'\n",
      "'  if (mapping != NULL) {' ---> 'why test the pointer? It cannot be NULL'\n",
      "'  if (mapping != NULL) {' ---> 'why test the pointer? It cannot be NULL'\n",
      "'  mapping->rlcMode = RLC_AM_MODE_' ---> 'Done'\n",
      "'  mapping->rlcMode = RLC_AM_MODE_' ---> 'Done'\n",
      "'  mapping->rlcMode = RLC_AM_MODE_' ---> 'the assignment of rlcMode is still redundant with RLC-Config code above'\n",
      "'  mapping->rlcMode = RLC_AM_MODE_' ---> 'the assignment of rlcMode is still redundant with RLC-Config code above'\n",
      "'  mapping->rlcMode = RLC_UM_MODE_' ---> 'Done'\n",
      "'  mapping->rlcMode = RLC_UM_MODE_' ---> 'Done'\n",
      "'  mapping->rlcMode = RLC_UM_MODE_' ---> 'the assignment of rlcMode is still redundant with RLC-Config code above'\n",
      "'  mapping->rlcMode = RLC_UM_MODE_' ---> 'the assignment of rlcMode is still redundant with RLC-Config code above'\n",
      "'  nr_drb_mapping_t *mapping = &nr_rrc_get_private_data(actx)->drb_mapping_' ---> 'Done'\n",
      "'  nr_drb_mapping_t *mapping = &nr_rrc_get_private_data(actx)->drb_mapping_' ---> 'Done'\n",
      "'  nr_drb_mapping_t *mapping = &nr_rrc_get_private_data(actx)->drb_mapping_' ---> 'Done'\n",
      "' * Editor modelines  -  http://www.wireshark.org/tools/modelines.html' ---> 'Done'\n",
      "' * Editor modelines  -  http://www.wireshark.org/tools/modelines.html' ---> 'Done'\n",
      "' * Editor modelines  -  http://www.wireshark.org/tools/modelines.html' ---> 'Done'\n",
      "' * Editor modelines  -  http://www.wireshark.org/tools/modelines.html' ---> 'https'\n",
      "' * Editor modelines  -  http://www.wireshark.org/tools/modelines.html' ---> 'https'\n",
      "' * Editor modelines  -  http://www.wireshark.org/tools/modelines.html' ---> 'https?'\n",
      "' * Editor modelines  -  http://www.wireshark.org/tools/modelines.html' ---> 'https?'\n",
      "'#if 0' ---> 'why \"#if 0\"?'\n",
      "'#if 0' ---> 'why \"#if 0\"?'\n",
      "'#include <QDebug>' ---> 'Done'\n",
      "'#include <QDebug>' ---> 'Done'\n",
      "'This menu item stops the currently running capture and starts again with' ---> 'Done'\n",
      "'This menu item stops the currently running capture and starts again with' ---> 'Done'\n",
      "'This menu item stops the currently running capture and starts again with' ---> 'This menu item needs to be removed to make the text consistent with the other entries.'\n",
      "'This menu item stops the currently running capture and starts again with' ---> 'This menu item needs to be removed to make the text consistent with the other entries.'\n",
      "'dissect_usbll_split(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, void* data _U_, gint offset)' ---> 'If you do plan to use it, you could leave it in if you want. However even in that case you would be modifying the prototype to remove the \"_U_\", so in the end it does not really matter.'\n",
      "'dissect_usbll_split(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, void* data _U_, gint offset)' ---> 'If you do plan to use it, you could leave it in if you want. However even in that case you would be modifying the prototype to remove the \"_U_\", so in the end it does not really matter.'\n",
      "'dissect_usbll_split(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, void* data _U_, gint offset)' ---> 'Yes, I've put it there because I've some further changes that are going to use it but it's not a problem to remove it and add in a future.'\n",
      "'dissect_usbll_split(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, void* data _U_, gint offset)' ---> 'Yes, I've put it there because I've some further changes that are going to use it but it's not a problem to remove it and add in a future.'\n",
      "'dissect_usbll_split(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, void* data _U_, gint offset)' ---> 'this _U_ means that the parameter is unused. Could you either remove this parameter from all other new, extracted functions, or remove the _U_ here? (I think the former would be cleaner)'\n",
      "'dissect_usbll_split(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, void* data _U_, gint offset)' ---> 'this _U_ means that the parameter is unused. Could you either remove this parameter from all other new, extracted functions, or remove the _U_ here? (I think the former would be cleaner)'\n"
     ]
    }
   ],
   "source": [
    "# duplicated lines and comments\n",
    "for header, row in pd.concat(g for _, g in wireshark_df.groupby([line_column, message_column]) if len(g) > 1).iterrows():\n",
    "    print(f\"'{row[line_column]}' ---> '{row[message_column]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:34:32.852400Z",
     "start_time": "2021-06-24T19:34:32.416430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message lengths distribution: 90% is 72, 95% is 107, 98% is 146, 99% is 186, and 100% is 333.0\n",
      "Your selected sequence length corresponds to 97.12 percentile in the training dataset.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 128\n",
    "comments_lengths = [len(tokenizer.encode(str(text))[0]) for text in wireshark_df[message_column].tolist()]\n",
    "print(\"Message lengths distribution: 90% is {:.0f}, 95% is {:.0f}, 98% is {:.0f}, 99% is {:.0f}, and 100% is {}\".format(\n",
    "        *np.percentile(comments_lengths, [90, 95, 98, 99, 100])))\n",
    "print(f\"Your selected sequence length corresponds to {stats.percentileofscore(comments_lengths, seq_len):.2f} percentile in the training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:38:40.469280Z",
     "start_time": "2021-06-24T19:38:39.920249Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from E:\\GoogleDrive\\acora-data\\onap\\onap_comments_all.xlsx\n",
      "Loaded 1,252 rows and 15 cols...\n"
     ]
    }
   ],
   "source": [
    "training_data_paths = [\n",
    "    os.path.join(data_path, \"onap\", \"onap_comments_all.xlsx\")\n",
    "]\n",
    "\n",
    "onap_df = load_comments_files(training_data_paths, cols, sep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:38:41.946250Z",
     "start_time": "2021-06-24T19:38:41.480248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments = 1252\n",
      "Number of unique comments = 959\n",
      "Number of duplicated lines and comments = 71\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of comments = {onap_df.shape[0]}\")\n",
    "print(f\"Number of unique comments = {onap_df[message_column].unique().shape[0]}\")\n",
    "print(f\"Number of duplicated lines and comments = {pd.concat(g for _, g in onap_df.groupby([line_column, message_column]) if len(g) > 1).shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:38:42.941252Z",
     "start_time": "2021-06-24T19:38:42.581259Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\t    dr = new DmaapPropertyReader(testinput);' ---> 'Done'\n",
      "'\t    dr = new DmaapPropertyReader(testinput);' ---> 'Done'\n",
      "'\t    dr = new DmaapPropertyReader(testinput);' ---> 'this is not used, please refactor this testcase'\n",
      "'\t    dr = new DmaapPropertyReader(testinput);' ---> 'this is not used, please refactor this testcase'\n",
      "'\texit' ---> 'tab'\n",
      "'\texit' ---> 'tab'\n",
      "'\texit' ---> 'tab'\n",
      "'\texit' ---> 'tab'\n",
      "'    ' ---> 'same'\n",
      "'    ' ---> 'same'\n",
      "'    ' ---> 'trailing white space'\n",
      "'    ' ---> 'trailing white space'\n",
      "'            e.printStackTrace();_' ---> 'Done'\n",
      "'            e.printStackTrace();_' ---> 'Done'\n",
      "'            if (status.toString().equals(\"FAILURE\"))' ---> 'use this instead: status == QueryStatus.FAILURE'\n",
      "'            if (status.toString().equals(\"FAILURE\"))' ---> 'use this instead: status == QueryStatus.FAILURE'\n",
      "'            rbs.executeProviderOperation(mg.getParams(), mg.getSvcLogicContext());' ---> 'maybe add an additional assertEquals to verify that REBOOT_STATUS=SUCCESS?'\n",
      "'            rbs.executeProviderOperation(mg.getParams(), mg.getSvcLogicContext());' ---> 'maybe add an additional assertEquals to verify that REBOOT_STATUS=SUCCESS?'\n",
      "'        Assert.assertTrue(302 == results.get().getResult().getStatus().getCode());' ---> 'Done'\n",
      "'        Assert.assertTrue(302 == results.get().getResult().getStatus().getCode());' ---> 'Done'\n",
      "'        Assert.assertTrue(302 == results.get().getResult().getStatus().getCode());' ---> 'use assertEquals instead'\n",
      "'        Assert.assertTrue(302 == results.get().getResult().getStatus().getCode());' ---> 'use assertEquals instead'\n",
      "'        String url = vm.getUrl();' ---> 'did something happen with the indentation?'\n",
      "'        String url = vm.getUrl();' ---> 'did something happen with the indentation?'\n",
      "'        assertEquals(result.getStatusCode(), AnsibleResultCodes.IO_EXCEPTION.getValue());' ---> 'swap arguments to assertEquals'\n",
      "'        assertEquals(result.getStatusCode(), AnsibleResultCodes.IO_EXCEPTION.getValue());' ---> 'swap arguments to assertEquals'\n",
      "'        fail(\"Exception not thrown\");' ---> 'Done'\n",
      "'        fail(\"Exception not thrown\");' ---> 'Done'\n",
      "'        fail(\"Exception not thrown\");' ---> 'delete this line'\n",
      "'        fail(\"Exception not thrown\");' ---> 'delete this line'\n",
      "'        try {' ---> 'no need to wrap this in a try/catch block'\n",
      "'        try {' ---> 'no need to wrap this in a try/catch block'\n",
      "'        try {_' ---> 'Done'\n",
      "'        try {_' ---> 'Done'\n",
      "'    if grep \"<module>_MVN_PROJECT_MODULEID</module>\" pom.xml >/dev/null' ---> 'ok'\n",
      "'    if grep \"<module>_MVN_PROJECT_MODULEID</module>\" pom.xml >/dev/null' ---> 'ok'\n",
      "'    logger' ---> 'Done'\n",
      "'    logger' ---> 'Done'\n",
      "'    public List<Transaction> generateSequence(SequenceGeneratorInput input) throws Exception {' ---> 'prefer: throws APPCException'\n",
      "'    public List<Transaction> generateSequence(SequenceGeneratorInput input) throws Exception {' ---> 'prefer: throws APPCException'\n",
      "'    public List<Transaction> generateSequence(SequenceGeneratorInput input) throws Exception {' ---> 'prefer: throws APPCException'\n",
      "'    stream:' ---> 'Done'\n",
      "'    stream:' ---> 'Done'\n",
      "'    }' ---> 'Added Verify.'\n",
      "'    }' ---> 'Added Verify.'\n",
      "'    }' ---> 'Added Verify.'\n",
      "'    }' ---> 'Fixed'\n",
      "'    }' ---> 'Fixed'\n",
      "'    }' ---> 'This looks good now.'\n",
      "'    }' ---> 'This looks good now.'\n",
      "'    }' ---> 'Yes. Fixed this in previous patch'\n",
      "'    }' ---> 'Yes. Fixed this in previous patch'\n",
      "'    }' ---> 'need to test anything?  Or is this sufficient as is?'\n",
      "'    }' ---> 'need to test anything?  Or is this sufficient as is?'\n",
      "'    }' ---> 'need to test anything?  Or is this sufficient as is?'\n",
      "'  if [ \"_BRANCH\" == \"master\" ]; then' ---> 'Note for the future we'll need an else if for beijing hopefully soon'\n",
      "'  if [ \"_BRANCH\" == \"master\" ]; then' ---> 'Note for the future we'll need an else if for beijing hopefully soon'\n",
      "' * Copyright (C) 2017 Amdocs' ---> 'If this is a new file, then remove the Amdocs lines'\n",
      "' * Copyright (C) 2017 Amdocs' ---> 'If this is a new file, then remove the Amdocs lines'\n",
      "' * Copyright (C) 2018 IBM' ---> 'It is correct that you added the IBM line, but the existing copyright lines should not be removed or modified.'\n",
      "' * Copyright (C) 2018 IBM' ---> 'It is correct that you added the IBM line, but the existing copyright lines should not be removed or modified.'\n",
      "' * Copyright (C) 2018 IBM' ---> 'It is correct that you added the IBM line, but the existing copyright lines should not be removed or modified.'\n",
      "' * ECOMP is a trademark and service mark of AT&T Intellectual Property.' ---> 'remove this line'\n",
      "' * ECOMP is a trademark and service mark of AT&T Intellectual Property.' ---> 'remove this line'\n",
      "' * ECOMP is a trademark and service mark of AT&T Intellectual Property.' ---> 'remove this line'\n",
      "' * ECOMP is a trademark and service mark of AT&T Intellectual Property.' ---> 'remove this line'\n",
      "' * ECOMP is a trademark and service mark of AT&T Intellectual Property.' ---> 'remove this line'\n",
      "'# ' ---> 'Done'\n",
      "'# ' ---> 'Done'\n",
      "'Previously database scripts were in ecomp-portal-BE/ecomp-portal-resources/sql scripts/' ---> 'Done'\n",
      "'Previously database scripts were in ecomp-portal-BE/ecomp-portal-resources/sql scripts/' ---> 'Done'\n"
     ]
    }
   ],
   "source": [
    "# duplicated lines and comments\n",
    "for header, row in pd.concat(g for _, g in onap_df.groupby([line_column, message_column]) if len(g) > 1).iterrows():\n",
    "    print(f\"'{row[line_column]}' ---> '{row[message_column]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:38:44.629248Z",
     "start_time": "2021-06-24T19:38:44.218250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message lengths distribution: 90% is 52, 95% is 75, 98% is 107, 99% is 126, and 100% is 257.0\n",
      "Your selected sequence length corresponds to 99.04 percentile in the training dataset.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 128\n",
    "comments_lengths = [len(tokenizer.encode(str(text))[0]) for text in onap_df[message_column].tolist()]\n",
    "print(\"Message lengths distribution: 90% is {:.0f}, 95% is {:.0f}, 98% is {:.0f}, 99% is {:.0f}, and 100% is {}\".format(\n",
    "        *np.percentile(comments_lengths, [90, 95, 98, 99, 100])))\n",
    "print(f\"Your selected sequence length corresponds to {stats.percentileofscore(comments_lengths, seq_len):.2f} percentile in the training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:39:07.955327Z",
     "start_time": "2021-06-24T19:39:07.811339Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from E:\\GoogleDrive\\acora-data\\mono\\mono-all.xlsx\n",
      "Loaded 172 rows and 15 cols...\n"
     ]
    }
   ],
   "source": [
    "training_data_paths = [\n",
    "    os.path.join(data_path, \"mono\", \"mono-all.xlsx\")\n",
    "]\n",
    "\n",
    "mono_df = load_comments_files(training_data_paths, cols, sep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:39:25.926003Z",
     "start_time": "2021-06-24T19:39:25.874003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments = 172\n",
      "Number of unique comments = 111\n",
      "Number of duplicated lines and comments = 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of comments = {mono_df.shape[0]}\")\n",
    "print(f\"Number of unique comments = {mono_df[message_column].unique().shape[0]}\")\n",
    "print(f\"Number of duplicated lines and comments = {pd.concat(g for _, g in mono_df.groupby([line_column, message_column]) if len(g) > 1).shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:40:02.429003Z",
     "start_time": "2021-06-24T19:40:02.376007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'public static void Main(string[] args)' ---> 'formatting: should use tabs not spaces, tabwidth=8'\n",
      "'public static void Main(string[] args)' ---> 'formatting: should use tabs not spaces, tabwidth=8'\n",
      "'}' ---> 'newline'\n",
      "'}' ---> 'newline'\n"
     ]
    }
   ],
   "source": [
    "# duplicated lines and comments\n",
    "for header, row in pd.concat(g for _, g in mono_df.groupby([line_column, message_column]) if len(g) > 1).iterrows():\n",
    "    print(f\"'{row[line_column]}' ---> '{row[message_column]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:40:19.388596Z",
     "start_time": "2021-06-24T19:40:19.310596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message lengths distribution: 90% is 61, 95% is 89, 98% is 155, 99% is 164, and 100% is 164.0\n",
      "Your selected sequence length corresponds to 95.35 percentile in the training dataset.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 128\n",
    "comments_lengths = [len(tokenizer.encode(str(text))[0]) for text in mono_df[message_column].tolist()]\n",
    "print(\"Message lengths distribution: 90% is {:.0f}, 95% is {:.0f}, 98% is {:.0f}, 99% is {:.0f}, and 100% is {}\".format(\n",
    "        *np.percentile(comments_lengths, [90, 95, 98, 99, 100])))\n",
    "print(f\"Your selected sequence length corresponds to {stats.percentileofscore(comments_lengths, seq_len):.2f} percentile in the training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T19:41:40.931710Z",
     "start_time": "2021-06-24T19:41:40.028709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message lengths distribution: 90% is 64, 95% is 94, 98% is 133, 99% is 164, and 100% is 333.0\n",
      "Your selected sequence length corresponds to 97.90 percentile in the training dataset.\n"
     ]
    }
   ],
   "source": [
    "seq_len = 128\n",
    "comments_lengths = [len(tokenizer.encode(str(text))[0]) for text in pd.concat([wireshark_df, onap_df, mono_df])[message_column].tolist()]\n",
    "print(\"Message lengths distribution: 90% is {:.0f}, 95% is {:.0f}, 98% is {:.0f}, 99% is {:.0f}, and 100% is {}\".format(\n",
    "        *np.percentile(comments_lengths, [90, 95, 98, 99, 100])))\n",
    "print(f\"Your selected sequence length corresponds to {stats.percentileofscore(comments_lengths, seq_len):.2f} percentile in the training dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-ml2",
   "language": "python",
   "name": "python-ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
